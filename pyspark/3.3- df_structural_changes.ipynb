{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca4cfad4-32b0-4b9e-98a0-5524656a4a82",
   "metadata": {},
   "source": [
    "## <mark> df.select().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44cc314-9a9a-4cf5-aea8-acea6b9fbb48",
   "metadata": {},
   "source": [
    "In PySpark, select() function is used to select single, multiple, column by index, all columns from the list and the nested columns from a DataFrame, PySpark select() is a transformation function hence it returns a new DataFrame with the selected columns.\n",
    "\n",
    "    df.select(\"firstname\",\"lastname\").show()\n",
    "    df.select(df.firstname,df.lastname)\n",
    "    df.select(df[\"firstname\"],df[\"lastname\"])\n",
    "\n",
    "    #By using col() function\n",
    "    from pyspark.sql.functions import col\n",
    "    df.select(col(\"firstname\"),col(\"lastname\"))\n",
    "\n",
    "    #Select columns by regular expression\n",
    "    df.select(df.colRegex(\"`^.*name*`\"))\n",
    "\n",
    "\n",
    "    # Select All columns from List\n",
    "    df.select(*columns)\n",
    "\n",
    "    # Select All columns\n",
    "    df.select([col for col in df.columns])\n",
    "    df.select(\"*\")\n",
    "\n",
    "    #Selects first 3 columns\n",
    "    df.select(df.columns[:3])\n",
    "\n",
    "    #Selects columns 2 to 4\n",
    "    df.select(df.columns[2:4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a94b594-9d28-4fef-88f2-ee077d0a016b",
   "metadata": {},
   "source": [
    "#### Select Nested Struct Columns\n",
    "\n",
    "    df2.select(\"name\")\n",
    "\n",
    "    +----------------------+\n",
    "    |name                  |\n",
    "    +----------------------+\n",
    "    |[James, Mac, Smith]   |\n",
    "\n",
    "    df2.select(\"name.firstname\",\"name.lastname\")\n",
    "\n",
    "    +---------+--------+\n",
    "    |firstname|lastname|\n",
    "    +---------+--------+\n",
    "    |James    |Smith   |\n",
    "    \n",
    "    df2.select(\"name.*\")\n",
    "    to get all columns from StuctType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d16d9f-332a-4152-a79e-b107cf3b5499",
   "metadata": {},
   "source": [
    "### <mark> df.collect()\n",
    "\n",
    "    deptDF.collect() returns Array of Row type.\n",
    "    deptDF.collect()[0] returns the first element in an array (1st row).\n",
    "    deptDF.collect[0][0] returns the value of the first row & first column.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797123ff-b118-474a-aa09-f1b897571c46",
   "metadata": {},
   "source": [
    "PySpark RDD/DataFrame collect() is **an action operation (not transformation) that is used to retrieve all the elements of the dataset (from all nodes) to the driver node.** We should use the collect() on **smaller dataset** usually after filter(), group() e.t.c. Retrieving larger datasets results in OutOfMemory error.\n",
    "    \n",
    "    dataCollect = deptDF.collect()\n",
    "    print(dataCollect)\n",
    "\n",
    "    [Row(dept_name='Finance', dept_id=10), \n",
    "    Row(dept_name='Marketing', dept_id=20), \n",
    "    Row(dept_name='Sales', dept_id=30), \n",
    "    Row(dept_name='IT', dept_id=40)]\n",
    "\n",
    "Note that collect() is an action hence it **does not return a DataFrame instead, it returns data in an Array** to the driver. Once the data is in an array, you can use python for loop to process it further.\n",
    "\n",
    "    for row in dataCollect:\n",
    "        print(row['dept_name'] + \",\" + str(row['dept_id']))\n",
    "        \n",
    "    # to return value of First Row, First Column which is \"Finance\"\n",
    "    deptDF.collect()[0][0]\n",
    "    \n",
    "In case you want to just return certain elements of a DataFrame, you should call PySpark select() transformation first.\n",
    "\n",
    "    dataCollect = deptDF.select(\"dept_name\").collect()\n",
    "    \n",
    "**select() is a transformation that returns a new DataFrame** and holds the columns that are selected whereas collect() is an action that returns the entire data set in an Array to the driver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feb0685-f7d6-4304-9964-1dba1a4d2103",
   "metadata": {},
   "source": [
    "### <mark> withColumn() \n",
    "    \n",
    "is a transformation function of DataFrame which is used to change the value, convert the datatype of an existing column, create a new column, and many more.\n",
    "    \n",
    "returns a new df instead of changing the original one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f551bca4-1c16-46b0-b34e-7f51d02aa750",
   "metadata": {},
   "source": [
    "change the datatype usign withColumn and cast()\n",
    "\n",
    "    df.withColumn(\n",
    "        \"salary\", \n",
    "        col(\"salary\").cast(\"Integer\")\n",
    "        )\n",
    "\n",
    "Update The Value of an Existing Column\n",
    "\n",
    "    df.withColumn(\"salary\", col(\"salary\")*100)\n",
    "        \n",
    "Create a Column from an Existing\n",
    "\n",
    "    df.withColumn(\"new_salary\", col(\"salary\")*1.5)\n",
    "        \n",
    "Add a New Column\n",
    "\n",
    "    df.withColumn(\"country\", lit('USA')) \\\n",
    "    .withColumn(\"planet\", lit('earth'))\n",
    "    \n",
    "Drop Column\n",
    "    \n",
    "    df.drop(\"salary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bcd80f-4c1b-406b-8a74-0dc2e2263754",
   "metadata": {},
   "source": [
    "### <mark> df.withColumnRenamed()\n",
    "    \n",
    "Since DataFrame’s are an immutable collection, you can’t rename or update a column instead when using withColumnRenamed() it **creates a new DataFrame with updated column names and doesn’t modify the current DataFrame.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869fb768-3ce3-4c80-bee5-452e119071d0",
   "metadata": {},
   "source": [
    "    df.withColumnRenamed(\"dob\",\"DateOfBirth\")\n",
    "\n",
    "    df2 = df.withColumnRenamed(\"dob\",\"DateOfBirth\") \\\n",
    "            .withColumnRenamed(\"salary\",\"salary_amount\") \\\n",
    "            .withColumnRenamed(\"fname\",\"first_name\") \\\n",
    "            .withColumnRenamed(\"lname\",\"last_name\")\n",
    "        \n",
    "When we have data in a flat structure (without nested) , use toDF() with a new schema to change all column names.\n",
    "    \n",
    "    newColumns = [\"newCol1\",\"newCol2\",\"newCol3\",\"newCol4\"]\n",
    "    df.toDF(*newColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefbb53c-7230-4c82-91c5-02d75696278f",
   "metadata": {},
   "source": [
    "### <mark> distinct() & dropDuplicates()\n",
    "\n",
    "PySpark distinct() function is used to **drop/remove the duplicate rows (all columns)** from DataFrame and dropDuplicates() is used to **drop rows based on selected (one or multiple) columns.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1253ad2-d579-4769-b077-aaee0d5538b4",
   "metadata": {},
   "source": [
    "    distinctDF = df.distinct()\n",
    "\n",
    "    df2 = df.dropDuplicates()\n",
    "\n",
    "    dropDisDF = df.dropDuplicates([\"department\",\"salary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b2e93e-5b7a-4db4-a45d-6bf8aa82fd66",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
