{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3158ff0-d87f-48ec-a87a-2920ca4bb297",
   "metadata": {},
   "source": [
    "# RDD: \n",
    "\n",
    "In Apache Spark, RDD (Resilient Distributed Datasets) is a fundamental data structure that represents a collection of elements, partitioned across the nodes of a cluster. RDDs can be created from various data sources, including Hadoop Distributed File System (HDFS), local file system, and data stored in a relational database.\n",
    "we create an RDD by calling the parallelize method on the SparkContext object\n",
    "\n",
    "    Can be easily converted to DataFrames and vice versa using the toDF() and rdd() methods.\n",
    "    Not type-safe\n",
    "    Low-level API with more control over the data, but lower-level optimizations compared to DataFrames and Datasets.\n",
    "    Provide full control over memory management, as they can be cached in memory or disk as per the user’s choice.\n",
    "    Whenever Spark needs to distribute the data within the cluster or write the data to disk, it does so use Java serialization. The overhead of serializing individual Java and Scala objects is expensive and requires sending both data and structure between nodes.\n",
    "    Provide a low-level API that requires more code to perform transformations and actions on data\n",
    "    Do not have an explicit schema, and are often used for unstructured data.\n",
    "    RDD APIs are available in Java, Scala, Python, and R languages. Hence, this feature provides flexibility to the developers.\n",
    "    No inbuilt optimization engine is available in RDD.\n",
    "    Suitable for structured and semi-structured data processing with a higher level of abstraction.\n",
    "    Suitable for low-level data processing and batch jobs that require fine-grained control over data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5020df52-f0d0-470f-b9c5-ea6d077d4d4c",
   "metadata": {},
   "source": [
    "# DataFrame\n",
    "\n",
    "In Spark Scala, a DataFrame is a distributed collection of data organized into named columns similar to an SQL table.\n",
    "It is similar to a table in a relational database or a spreadsheet in that it has a schema, which defines the types and names of its columns, and each row represents a single record or observation.\n",
    "DataFrames in Spark Scala can be created from a variety of sources, such as RDDs, structured data files (e.g., CSV, JSON, Parquet), Hive tables, or external databases\n",
    "Once created, DataFrames support a wide range of operations and transformations, such as filtering, aggregating, joining, and grouping data.\n",
    "One of the key benefits of using DataFrames in Spark Scala is their ability to leverage Spark’s distributed computing capabilities to process large amounts of data quickly and efficiently.\n",
    "Overall, DataFrames in Spark provides a powerful and flexible way to work with structured data in a distributed computing environment.\n",
    "\n",
    "    Can be easily converted to RDDs and Datasets using the rdd() and as[] methods respectively.\n",
    "    DataFrames are not type-safe, When we are trying to access the column which does not exist in the table in such case Dataframe APIs does not support compile-time error. It detects attribute errors only at runtime\n",
    "    Optimized for performance, with high-level API, Catalyst optimizer, and code generation.\n",
    "    Have more optimized memory management, with a Spark SQL optimizer that helps to reduce memory usage.\n",
    "    DataFrames use a generic encoder that can handle any object type.\n",
    "    Provide a high-level API that makes it easier to perform transformations and actions on data.\n",
    "    DataFrames enforce schema at runtime. Have an explicit schema that describes the data and its types.\n",
    "    Available In 4 languages like Java, Python, Scala, and R.\n",
    "    It uses a catalyst optimizer for optimization.\n",
    "    DataFrames supports most of the available dataTypes\n",
    "    Suitable for structured and semi-structured data processing with a higher-level of abstraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e393a61-e94a-4a8e-897e-4b221c525925",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "A Dataset is a distributed collection of data that provides the benefits of strong typing, compile-time type safety, and object-oriented programming. It is essentially a strongly-typed version of a DataFrame, where each row of the Dataset is an object of a specific type, defined by a case class or a Java class.\n",
    "One of the key benefits of using Datasets in Spark Scala is their ability to provide compile-time type safety and object-oriented programming, which can help catch errors at compile time rather than runtime. This can help improve code quality and reduce the likelihood of errors.\n",
    "\n",
    "    Can be easily converted to DataFrames using the toDF() method, and to RDDs using the rdd() method.\n",
    "    Datasets are type-safe, Datasets provide compile-time type checking, which helps catch errors early in the development process. DataFrames are schema-based, meaning that the structure of the data is defined at runtime and is not checked until runtime.\n",
    "    Datasets are faster than DataFrames because they use JVM bytecode generation to perform operations on data. This means that Datasets can take advantage of the JVM’s optimization capabilities, such as just-in-time (JIT) compilation, to speed up processing.\n",
    "    support most of the available dataTypes\n",
    "    Datasets are serialized using specialized encoders that are optimized for performance.\n",
    "    Datasets provide a richer set of APIs. Datasets support both functional and object-oriented programming paradigms and provide a more expressive API for working with data\n",
    "    Datasets enforce schema at compile time. With Datasets, errors in data types or structures are caught earlier in the development cycle. Have an explicit schema that describes the data and its types, and is strongly typed.\n",
    "    Only available in Scala and Java.\n",
    "    It includes the concept of a Dataframe Catalyst optimizer for optimizing query plans.\n",
    "    Datasets support all of the same data types as DataFrames, but they also support user-defined types. Datasets are more flexible when it comes to working with complex data types.\n",
    "    Suitable for high-performance batch and stream processing with strong typing and functional programming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef2948f-9c34-499d-9fdc-8c850a59db6a",
   "metadata": {},
   "source": [
    "| . | RDD | DataFrame | Dataset |\n",
    "|:----------:|:----------:|:----------:|:----------:|\n",
    "|API|Low level|High level|High level|\n",
    "||requires more code|optimized code|more expressive API|\n",
    "|Language support|In 4 languages|In 4 languages|Only in Java/Scala|\n",
    "|Type safety|NO|At run time|At compile time|\n",
    "||for more control over data|optimized for performance, uses catalyst optimizer|faster then DFs because uses JVM's bytecode generation|\n",
    "|Schema enforcement|Do not have an explicit schema, and are often used for unstructured data.|DataFrames enforce schema at runtime.|Datasets enforce schema at compile time.(strongly typed)|\n",
    "|Optimization|No inbuilt optimization engine|uses a catalyst optimizer|uses Dataframe Catalyst optimizer|\n",
    "|Data types|Suitable for structured and semi-structured data| supports most of the available dataTypes|support all of the same data types as DF, as well as user defined datatypes|\n",
    "|Use Cases|Suitable for low-level data processing and batch jobs that require fine-grained control over data|Suitable for structured and semi-structured data processing with a higher-level of abstraction.|Suitable for high-performance batch and stream processing with strong typing and functional programming.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158a6efb-bd6c-42d6-b4ba-4ffd49613ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
