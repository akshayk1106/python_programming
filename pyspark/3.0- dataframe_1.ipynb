{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d036ae-dcde-46f6-aa39-eedcad9065e5",
   "metadata": {},
   "source": [
    "### <mark>Create empty DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789badfb-2bc1-41d3-8acc-6e772e8f1651",
   "metadata": {},
   "source": [
    "While working with files, sometimes we may not receive a file for processing, however, we still need to create a DataFrame manually with the same schema we expect. If we don’t create with the same schema, our operations/transformations (like union’s) on DataFrame fail as we refer to the columns that may not present. A DataFrame with the same schema, which means the same column names and datatypes regardless of the file exists or empty file processing. If you try to perform operations on empty RDD you going to get ValueError(\"RDD is empty\").\n",
    "\n",
    "\n",
    "    # Creates Empty RDD using emptyRDD()\n",
    "    emptyRDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "    # Creates Empty RDD using parallelize([])\n",
    "    rdd2= spark.sparkContext.parallelize([])\n",
    "    \n",
    "    # Create Empty DataFrame with Schema (StructType) and RDD\n",
    "    from pyspark.sql.types import StructType,StructField, StringType\n",
    "    schema = StructType([StructField('firstname', StringType(), True),])\n",
    "    emptyRDD = spark.sparkContext.emptyRDD()\n",
    "    df = spark.createDataFrame(emptyRDD, schema)\n",
    "    # Convert empty RDD to Dataframe with Schema\n",
    "    df1 = emptyRDD.toDF(schema)\n",
    "\n",
    "    # Create Empty DataFrame with Schema and without RDD.\n",
    "    df2 = spark.createDataFrame([], schema)\n",
    "\n",
    "    # Create Empty DataFrame without Schema (no columns) and without RDD\n",
    "    df3 = spark.createDataFrame([], StructType([]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e320d09-fdd1-49eb-9625-ebf16c17c0a1",
   "metadata": {},
   "source": [
    "### <mark> Convert PySpark RDD to DataFrame\n",
    "\n",
    "In PySpark, toDF() function of the RDD is used to convert RDD to DataFrame. We would need to convert RDD to DataFrame as DataFrame provides **more advantages over RDD.** For instance, **DataFrame is a distributed collection of data organized into named columns similar to Database tables and provides optimization and performance improvements.**\n",
    "\n",
    "In PySpark, **when you have data in a list meaning you have a collection of data in a PySpark driver memory when you create an RDD, this collection is going to be parallelized.**\n",
    "\n",
    "    dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "    rdd = spark.sparkContext.parallelize(dept)\n",
    "\n",
    "Converting PySpark RDD to DataFrame can be done using **toDF(), createDataFrame().**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4470d08c-0626-4752-bd5a-7036c91b11bc",
   "metadata": {},
   "source": [
    "By default, toDF() function creates column names as “_1” and “_2”.\n",
    "> df = rdd.toDF()\n",
    "\n",
    "the names of columns can be passed as list\n",
    "> deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "> df2 = rdd.toDF(deptColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc6869e-0470-401b-bf4d-d6086ec4fddf",
   "metadata": {},
   "source": [
    "SparkSession class provides createDataFrame() method to create DataFrame and it takes rdd object as an argument.\n",
    "    \n",
    "> deptDF = spark.createDataFrame(rdd, schema = deptColumns)\n",
    "    \n",
    "When you infer the schema, by default the datatype of the columns is derived from the data and set’s nullable to true for all columns. We can change this behavior by supplying schema using StructType – where we can specify a column name, data type and nullable for each field/column.\n",
    "\n",
    "> from pyspark.sql.types import StructType,StructField, StringType\n",
    "\n",
    "        deptSchema = StructType([       \n",
    "            StructField('dept_name', StringType(), True),\n",
    "            StructField('dept_id', StringType(), True)\n",
    "        ])\n",
    "\n",
    "> deptDF1 = spark.createDataFrame(rdd, schema = deptSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3693e173-f9ff-4052-bc6f-87789322803d",
   "metadata": {},
   "source": [
    "### <mark> Convert PySpark DataFrame to Pandas\n",
    "\n",
    "operations on Pyspark run faster than Pandas due to its distributed nature and parallel execution on multiple cores and machines. After processing data in PySpark we would need to convert it back to Pandas DataFrame for a further procession with Machine Learning application or any Python applications.\n",
    "    \n",
    "PySpark DataFrame provides a method **toPandas()** to convert it to Python Pandas DataFrame.\n",
    "\n",
    "**toPandas() results in the collection of all records in the PySpark DataFrame to the driver program** and should be done only on a small subset of the data. running on larger dataset’s results in **memory error and crashes**  the application. To deal with a larger dataset, you can also try increasing memory on the driver.\n",
    "\n",
    "pandas add a sequence number to the result as **a row Index.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddebefa-b822-4f78-a7b8-1f33d66ea39f",
   "metadata": {},
   "source": [
    "    data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n",
    "            (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000)]\n",
    "\n",
    "    columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n",
    "    pysparkDF = spark.createDataFrame(data = data, schema = columns)\n",
    "    pandasDF = pysparkDF.toPandas()\n",
    "    \n",
    "    pysparkDF:\n",
    "    +----------+-----------+---------+-----+------+------+\n",
    "    |first_name|middle_name|last_name|dob  |gender|salary|\n",
    "    +----------+-----------+---------+-----+------+------+\n",
    "    |James     |           |Smith    |36636|M     |60000 |\n",
    "    |Michael   |Rose       |         |40288|M     |70000 |\n",
    "    \n",
    "    pandasDF:\n",
    "      first_name middle_name last_name    dob gender  salary\n",
    "    0      James                 Smith  36636      M   60000\n",
    "    1    Michael        Rose            40288      M   70000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dadece3-0a77-4f1d-84b3-f1f9ad0c1306",
   "metadata": {},
   "source": [
    "### <mark> Convert Spark Nested Struct DataFrame to Pandas\n",
    "    \n",
    "Most of the time data in PySpark DataFrame will be in a structured format meaning one column contains other columns so let’s see how it convert to Pandas. Here is an example with nested struct where we have firstname, middlename and lastname are part of the name column.\n",
    "\n",
    "    # Nested structure elements\n",
    "    from pyspark.sql.types import StructType, StructField, StringType,IntegerType\n",
    "    name_data = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",\"3000\"), \\\n",
    "          ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",\"4000\")]\n",
    "\n",
    "    schemaStruct = StructType([\n",
    "            StructField('name', StructType([\n",
    "                 StructField('firstname', StringType(), True),\n",
    "                 StructField('middlename', StringType(), True),\n",
    "                 StructField('lastname', StringType(), True)\n",
    "                 ])),\n",
    "             StructField('dob', StringType(), True),\n",
    "             StructField('gender', StringType(), True),\n",
    "             StructField('salary', StringType(), True)\n",
    "             ])\n",
    "    df = spark.createDataFrame(data=name_data, schema = schemaStruct)\n",
    "    pandasDF2 = df.toPandas()\n",
    "\n",
    "                       name    dob gender salary\n",
    "    0      (James, , Smith)  36636      M   3000\n",
    "    1     (Michael, Rose, )  40288      M   4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f67e3f9-8692-4398-8813-4a8996271077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f524782e-e3d5-41f0-826d-6fd37235e840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
